# generate_llm.py

from llama_cpp import Llama
from query import search_chunks
import argparse

# -----------------------
# üîß LLM Model Configurations
# -----------------------
MODEL_CONFIG = {
    "mistral": {
        "path": "models/mistral-7b-instruct-v0.1.Q5_K_M.gguf",
        "n_gpu_layers": 0,
    },
    "medllama": {"path": "models/MedLLaMA-3.Q4_K_M", "n_gpu_layers": 20},
}


# -----------------------
# üß† Prompt Construction
# -----------------------
def build_prompt(query, chunks, model_name):
    context = "\n\n".join(
        f"File: {chunk['file']} | Chunk ID: {chunk['chunk_id']}\n{chunk['preview']}"
        for chunk in chunks
    )

    system_prompt = (
        "You are a helpful clinical assistant specialized in CVICU nursing policies. "
        "Use the provided policy excerpts to answer the user's question as accurately and completely as possible. "
        "Only use relevant clinical information such as administration, dosage, monitoring, adverse effects, precautions, and indications. "
        "When asked about dosage, give the exact dosing ranges, units, and route (e.g., IV, subq), and include frequency. "
        "Always include the patient context (e.g., ICU, palliative). If a range is provided, give both minimum and maximum. "
        "If multiple administration methods exist, explain each. Respond clearly and professionally. "
        "Ignore any content related to legal disclaimers, institutional boilerplate, headers, page numbers, or footnotes. "
        "If the answer is not directly stated, use your best clinical judgment based on the context."
    )

    # Change format based on model
    if "medllama" in model_name.lower():
        return f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer: [/INST]"
    else:
        return f"[INST] {system_prompt}\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer: [/INST]"


# -----------------------
# üîÑ Load LLM Model
# -----------------------
def load_llm(model_name):
    config = MODEL_CONFIG[model_name]
    print(f"üîÑ Loading {model_name} model from {config['path']}...")

    llm = Llama(
        model_path=config["path"],
        n_ctx=2048,
        n_threads=8,
        n_gpu_layers=config["n_gpu_layers"],
        verbose=False,
    )

    print(f"‚úÖ {model_name} model loaded!\n")
    return llm


# -----------------------
# ü©∫ Run Query with LLM
# -----------------------
def generate_answer(llm, query):
    top_chunks = search_chunks(query)
    prompt = build_prompt(query, top_chunks)

    print("\nüß† Prompt Sent to LLM:\n")
    print(prompt)

    output = llm(prompt, max_tokens=350, temperature=0.7)
    answer = (
        output.get("choices", [{}])[0]
        .get("text", "[No response generated by model]")
        .strip()
    )

    if not answer:
        answer = "[‚ö†Ô∏è Model returned no answer. You may need to increase context or check prompt.]"

    print("\nüìö Sources:")
    for chunk in top_chunks:
        print(f"- {chunk['file']} (Chunk ID: {chunk['chunk_id']})")

    print("\nü©∫ Generated Answer:\n")
    print(answer)


# -----------------------
# üß™ CLI Entry Point
# -----------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model",
        type=str,
        default="mistral",
        choices=MODEL_CONFIG.keys(),
        help="Which model to use",
    )
    args = parser.parse_args()

    llm = load_llm(args.model)

    while True:
        query = input("\nAsk a clinical question (or type 'exit'): ")
        if query.lower() == "exit":
            break
        generate_answer(llm, query)
